{"0": {
    "doc": "Introduction to Pragmatic Formal Modeling",
    "title": "Introduction to Pragmatic Formal Modeling",
    "content": "{% include title_toc.md %} ## Introduction A [formal model](https://en.wikipedia.org/wiki/Formal_methods) is a mathematical description of a system, generally to be implemented in hardware or software. They are useful for two reasons. Firstly, mathematics, unlike English, is precise and unambiguous. Even if this is where you stop, it forces you to understand the system you are describing. Secondly, mathematically based models can be checked. You can describe success criteria, and if they are violated you can see the exact series of steps that led to that. This is particularly useful for distributed systems, from multiple threads on a computer to thousands of computers in a cloud service. When you think about formal modeling, it's easy (and intimidating) to jump straight to the most complex use cases. Sure, those algorithm geniuses who design the fundamental algorithms of distributed systems might need it, but what about me? I care about the quality of my work. But I work in industry. Maybe I work in the cloud, coordinating microservices. Maybe I'm a game developer writing the next multiplayer networking library. Maybe I'm building a peer to peer file storage solution. Regardless, the question remains: >How can I model the system I'm building in a way that's useful, practical, and has good ROI for myself and my company? For every example on this site, we're going to try to take a pragmatic approach that mirrors the engineering design lifecycle. We'll start with UML diagrams and relatively precise descriptions, and then convert them into a formal specification language. Then we'll see how we can check for design errors, and get concrete examples as to how they occur. Finally we will show how to use the detailed model errors to progressively refine your designs. ## The modeling and model testing language TLA+ is a specification language developed by Leslie Lamport, one of the computer science greats. It has a model checker: TLC, which works in a brute force manner to check every possible state of your modelled system. [TLA+ is used in industry](https://lamport.azurewebsites.net/tla/industrial-use.html). [AWS is one of its most enthusiastic users](https://cacm.acm.org/magazines/2015/4/184701-how-amazon-web-services-uses-formal-methods/fulltext), along with Microsoft and Intel. While it looks very math-y, it is surprisingly accessible and practical. TLA+ was designed as a tool for engineers, not just algorithmists. But current examples tend to fall into one of two categories: toy problems, or complex algorithms. One of my goals with this site is to present examples that show how it can be used in an engineering process for \"normal\" engineers. My advice is to skim the examples and see if they resonate. Could this help you in your day-to-day? Does it pique your interest? If so, go through the [learning material](learning-material) and revisit this post. However if you are mathematically inclined and obsessed with complex distributed system algorithms already, I'd advise you just jump straight to the TLA+ Video Series: [Leslie Lamport will be able to sell you better than I can](https://lamport.azurewebsites.net/video/videos.html). ## I want to run the examples without looking at the [learning material](learning-material)! Ok, here's how: First download [VSCode TLA+](https://marketplace.visualstudio.com/items?itemName=alygin.vscode-tlaplus) For each example you want to run: 1. Click _Download Code_. The file will look like \"modelfoo.tla\" 2. Scroll down (or hit _Next Section_), where you will see a _Download Configuration_ link. Click it. The file will look like \"modelfoo.cfg\" 3. Place both files in the same folder. Make sure they have the same name (other than the extension). If necessary rename \"modelfoo_small.cfg\" to \"modelfoo.cfg\" 4. Open the folder in VSCode and open the TLA file 5. Right click inside the editor and click _Check Model with TLC_ 6. An output window will open and you will get a result. _Or you will get an error, probably because you didn't rename the .cfg file_ For any example that relies on additional model files, simply download them and place them in the same folder before clicking _Check Model with TLC_. ## What does modeling get us? (A Simple Example) We will start with a toy problem, Tic-Tac-Toe, just to demonstrate some of the core benefits of modeling and model checking. This is the only toy problem, so feel free to [skip ahead to the practical part](database-blob). If you want to get a bit more of an intuitive understanding, keep reading. ### The Model _I advise that you look at the code both in LaTex and code form. The LaTex is generated directly from the code and can let you appreciate the math a bit better._ {% include code.html path=\"tictactoe/1everygame/tictactoe\" %} Essentially, this model describes every game of Tic-Tac-Toe that could be played, including games where someone has already won, but the board isn't full. The model checker actually shows us how many possible board/move combinations exist: {% include states.md states=page.fullstate namespace=\"fullstate\" modelcfg=\"tictactoe/1everygame/tictactoe-full.cfg\" %} It ends when there are no more spaces left to fill. None of the Next actions are activated, so the system \"deadlocks\". Sometimes this behavior is an error, but for us it's a feature. You can configure how to treat it. ### Checking the Model All right, that's kind of cool, but so what? Say that it would be really bad if **O** were to win. We can check a model against the _OHasNotWon_ invariant to make sure it's impossible for **O** to win. Let's run that now. {% include trace.html traceconfig=page.owin constraint=\"Oh no, it is possible for **O** to win. Invariant OHasNotWon is violated.\" trace=site.data.tictactoe.1everygame.owin modelcfg=\"tictactoe/1everygame/tictactoe-owin.cfg\" %} Now the interesting thing is not that **O** can win Tic-Tac-Toe. We probably knew that. But the model checker uses Breadth-First-Search, so not only is this a possible **O** victory, there are no faster victories. What's also interesting is how clearly it is presented. The data above is EXACTLY what comes out of the model checker, prettied up with generic css and a bit of annotation. Look how much clearer it is than standard a code debugger. Because we're not debugging code, we're debugging logic. Let's see what happens when **X** wins. {% include trace.html traceconfig=page.xwin constraint=\"X can also win. Invariant XHasNotWon is violated.\" trace=site.data.tictactoe.1everygame.xwin modelcfg=\"tictactoe/1everygame/tictactoe-xwin.cfg\" %} Finally, let's look at a stalemate. In those past wins, it looked like the other player wasn't trying very hard. That's because we saw the fastest possible wins. Let's see a potential stalemate: {% include trace.html traceconfig=page.stalemate constraint=\"Stalemate is also a thing that can happen in Tic-Tac-Toe. Invariant NotStalemate is violated.\" trace=site.data.tictactoe.1everygame.stalemate modelcfg=\"tictactoe/1everygame/tictactoe-stalemate.cfg\" %} So **O** blocked an **X** win here, but there is no intelligence yet. There is another world where **O** didn't block **X**, and it was an **X** win. This is just one of the possible stalemates. So in our current system it will be possible for **O** to win. What can we do to fix that? ### Playing not to lose Let's imagine it is really important that **O** never win. If they do, the casino you work for loses millions of dollars (why they introduced high-stakes Tic-Tac-Toe is above our pay grade). **O** will still play every possible game available to it. But the casino is **X**, meaning we can change its strategy. How do we do that? We put stricter limits on what is considered an allowable move for **X**: - The previous version of MoveX let **X** be put into any unfilled space. - In this updated version, a programmer read the WikiHow on \"How to Play Tic-Tac-Toe\" and made a best attempt at a strategy. We need to prove the **O** will not win if this strategy is used. We do this by encoding it to TLA+ and running the same check (to see the full implementation, click _Download Code_ or _Download PDF_): {% include code.html path=\"tictactoe/2xstrategy/tictactoexstrat\" snippet=\"tictactoe/2xstrategy/tictactoexstrat-snippet\" %} Let's see what happens when we set it up with invariant OHasNotWon: {% include states.md states=page.xstratstate namespace=\"xstratstate\" modelcfg=\"tictactoe/2xstrategy/tictactoexstrat.cfg\"%} We didn't get an error trace! Normally that's a cause for suspicion; did we mess up the test? But by building incrementally we can have more confidence: we've seen this test fail. So we did it! **O** wil never win. So what exactly does that mean? The strategy isn't optimal (i.e. wins in the smallest number of moves) or deterministic. For any given turn it may allow multiple moves. The TLC model checker tested all of those moves and ensured that **O** would never win, no matter which of those moves we pick. This is true no matter what **O** does. We will call all moves allowed by the strategy _Strategy Moves._ If we wanted to maximize winning, we could run a machine learning algorithm that tries to predict which move would lead to victory. It doesn't even need to be completely logical, it could try to psych out its opponent, or realize that people wearing red shirts are more likely to move left. As long as it only picks moves that are _Strategy Moves_ we will never lose. {% include_relative tictactoe/2xstrategy/safeoptions.svg %} This is a silly example. But combining machine learning with logical safeguards designed and tested with TLA+ has a lot of potential. TLA+ is logical modeling; it can't give you statistical optimization or tell you how many 9s of reliability you will get. But it can let you update algorithms confidently, knowing that critical parameters will be met. ### Playing to win \"So you stopped us from losing,\" says your boss, \"but that was yesterday, and anyway, we're a company of winners.\" \"Right on it boss,\" you respond, because you know what to do next. To prove that our system is going to win, we have to first describe winning. Let's start with what we already know. {% highlight tla %} Won(\"X\") is true when X has Won []Won(\"X\") means always Won(\"X\"). {% endhighlight %} But at the start of the game you haven't won. This would fail. We need a new operator. {% highlight tla %} Won(\"X\") means Won(\"X\") must EVENTUALLY be true {% endhighlight %} This is what's called a _Temporal Property_, a property that is measured across steps, and we can test for it. **X** eventually winning sounds exactly like what we want. > _If a little thing called \"Eventual Consistency\" is important to you, don't worry, you'll see this again!_ So let's test our code. >.\"> We get an error, but it's unlike any we've seen before. What is a _Stuttering Step_? Well, so far we've been thinking about this one state after another. But another valid thing that can happen is nothing. And nothing can happen forever. There's a world where one of the participants just walks away from the game board, which means X will never win. How cool is it that our modeling tool can point that out to us? But don't worry, our casino won't allow that to happen. If a player can take a move, they will eventually take a move. We say that formally with the concept of Fairness. Weak Fairness (represented as WF) roughly means that if a move can be made, and that fact doesn't change, the move will eventually be made. We define it below. {% include code.html path=\"tictactoe/3xwin/tictactoexwin\" snippet=\"tictactoe/3xwin/tictactoexwin-snippet\" %} > _Note: Fairness isn't something we are checking for, it's a property of how the system works._ Great, let's run it again. {% include trace.html traceconfig=page.stratstalement constraint=\"We run into a stalemate. Which means that X won't always eventually win.\" trace=site.data.tictactoe.3xwin.stalemate modelcfg=\"tictactoe/3xwin/tictactoexwin.cfg\" %} So we didn't succeed. X does not always eventually win. The house may not always win, but it never loses. We can live with that. But if you figured out an algorithm to make X always win, this is a way to prove it. > _Tic-Tac-Toe is a solved problem, and there is no such algorithm, but don't let that limit you._ |Next: [Coordinating a Database and Blob Store](database-blob)| ",
    "url": "http://localhost:4000/pragmaticformalmodeling/",
    "relUrl": "/"
  },"1": {
    "doc": "(Implementing New Requirements) Significant improvement",
    "title": "(Implementing New Requirements) Significant improvement",
    "content": "# {{page.title}} {: .no_toc } 1. TOC {:toc} ## Refining the design How do we prevent the cleaner from deleting items just as they are being written? Well, the common sense solution is to check the creation time. We should only start cleaning a key after a safe window of time has passed since its creation. Let's call it 2 hours. ### Storage Cleaner Run ```plantuml @startuml participant \"Storage Cleaner\" as StorageCleaner participant Database participant \"Blob Store\" as BlobStore StorageCleaner -> BlobStore: Gets batch of keys that have been created more than or exactly 2 hours ago StorageCleaner Database : Queries for unused keys StorageCleaner BlobStore: Batch deletes unused keys StorageCleaner StorageCleaner: Repeats from beginning end @enduml ``` ### Assumptions We are making one main assumption: that all of our clocks are accurate within a reasonable margin of error (5 minutes is generous), and our code errs on the side of not deleting based on those margins. While you can't always make assumptions about clock time in distributed systems, in this case our time frames are so large (hours) that it's probably not a bad assumption. Note: this doesn't mean a program will necessarily check its clock. It could stall and then resume what it's doing an hour later. ## Modeling the design Now we have to introduce a concept of time in the model. You might think we've been using time throughout this whole tutorial, but actually, we were just using ordering. We will need to add the concept of creation time to the blob store. Keep in mind that the state diagram has not changed. > _Note: This isn't adding functionality. We're just modeling details that have become relevant._ {% include code.html path=\"storagecleanerimproved\" %} ## Verifying Storage Cleaner We're going to start off with the slightly larger model (two servers and two cleaners), since the last test didn't show behavioral differences. Might as well perform the more rigorous test. {% include trace.html traceconfig=page.trace constraint=\"Invariant ConsistentReads is violated.\" trace=site.data.database-blob.storage-cleaner-improved.trace modelcfg=\"storagecleanerimproved.cfg\" namespace=\"trace\" %} The error we saw previously is gone, implying we fixed the design flaw we hoped to fix. This new error is much more complex. It requires the **Server** to stall at just the wrong time and be out of commission for 2 hours. This is pretty unlikely; in fact, it's unlikely enough that some companies might be okay with it. But the fix is obvious: kill the servers after 1 hour of stalling or less. Chances are we were going to do it anyway in implementation, but let's model it to get the extra assurance. ## A quick fix All the changes in the model are in the server behavior. Despite the large number of changes, all we're really saying is that if less than an hour has passed since the server request started, it can proceed to the next state. Otherwise, it proceeds to the restart state. This can be reflected in an updated state diagram for Server writes: ```plantuml @startuml hide empty description [*] --> Waiting Waiting --> StartWrite: Assigns start time StartWrite --> WriteMetadata StartWrite --> ServerRestart: timeout case StartWrite --> FailWrite WriteMetadata --> WriteBlobAndReturn WriteMetadata --> ServerRestart: timeout case WriteMetadata --> FailWrite WriteBlobAndReturn --> Waiting ServerRestart --> Waiting: timeout case @enduml ``` This is reflected in the code below. {% include code.html path=\"storagecleanerimprovedkillsnippet\" %} Looks good, right? Let's test it. {% include trace.html traceconfig=page.kill constraint=\"Close but no cigar. Invariant ConsistentReads is violated.\" trace=site.data.database-blob.storage-cleaner-improved.kill namespace=\"kill\" %} We've gotten closer, and our error is even more obscure. Now it requires an interaction of **Storage Cleaner**, a **Read Server**, and a **Write Server**. But there's a relatively simple fix. > _Note: This was the first error that I did not expect. But it is exciting that TLA+ wouldn't let me get away with it!_ | Next: [(Implementing New Requirements) A working update](../storage-cleaner-working) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/database-blob/storage-cleaner-improved/",
    "relUrl": "/database-blob/storage-cleaner-improved/"
  },"2": {
    "doc": "(Implementing New Requirements) A working update",
    "title": "(Implementing New Requirements) A working update",
    "content": "# {{page.title}} {: .no_toc } 1. TOC {:toc} ## A last fix Our remaining problem is that Read operations can hang for an hour and thus have references to images that are out of date and have been deleted. We can fix this by introducing a delay of 1 hour between getting the unused keys and deleting them. ### Storage Cleaner Run ```plantuml @startuml participant \"Storage Cleaner\" as StorageCleaner participant Database participant \"Blob Store\" as BlobStore StorageCleaner -> BlobStore: Gets batch of keys that have been created more than or exactly 2 hours ago StorageCleaner Database : Queries for unused keys StorageCleaner StorageCleaner: Waits for at least 1 hour StorageCleaner -> BlobStore: Batch deletes unused keys StorageCleaner StorageCleaner: Repeats from beginning end @enduml ``` ## Modeling the design This is a relatively simple update to **Cleaner**. The relevant snippet is shown below. Click _Download Code_ or _Download PDF_ to see the entire program. {% include code.html path=\"storagecleaner\" snippet=\"storagecleaner-snippet\" %} ## Verifying Storage Cleaner This is the largest simulation we can comfortably run. We've seen errors that require at least a cleaner and two servers, so we know we can't go any smaller. {% highlight tla %} SPECIFICATION Spec CONSTANTS \\* Defined as symmetry sets to make the problem tractable SERVERS = {s1, s2} CLEANERS = {c1, c2} METADATAS = {m1, m2} USERIDS = {u1} IMAGES = {i1, i2} UUIDS = {ui1, ui2, ui3} CONSTRAINT StopAfter3Operations INVARIANT TypeOk ConsistentReads PROPERTY AlwaysEventuallyNoOrphanFiles {% endhighlight %} After an hour, the model checker returns successfully. {% include states.md states=page.states namespace=\"state\" modelcfg=\"storagecleaner.cfg\"%} Because this same model caught a number of obscure bugs previously, we can feel reasonably confident in the fact that it passed now. ## Great! But how many 9s will it have? Now that we've gone through the modeling task and tested correctness, it might be tempting to say that it will work 100% of the time in practice. That's obviously not true, but going through the modeling process can help us with estimation. First we need to understand the metric(s) we've been testing. In this case, the metric is data consistency. And what we want is a lower bound of per request reliability. For each read/write operation, what is the likelihood of a consistent read response or consistency being maintained on write? Let's say we trust our model, which means we assign a 100% likelihood of every behavior we model working. What's left? Our assumptions. Here are the assumptions we've made. We can assign per request probabilities that they will malfunction: - **Our clocks are accurate enough to handle hour time frames:** - The likelihood of cloud services (blob store and server management solution) having their time off by an hour is absurdly low. - Let's guess **99.9999999999%** it will not occur in any given request. - We'll be pessimistic and assume clock inaccuracies will cause request failure. - **Servers will not hang for > 1 hour:** - Assume servers have 99.9% uptime and requests are distributed evenly: 99.9% chance a request is not delayed. - Assume our server monitoring solution has 99.9% uptime. - Assume servers fail independently (which is less representative the more applications are clustered on one physical machine or in one region). - **99.9999%** chance that any given request won't hang. - We'll be pessimistic and assume any server that hangs and is not killed will cause request failure. Let's take a look at some implicit assumptions we made: - **Our database does not corrupt records:** - Heroku Postgres has **99.999999999%** durability, so let's use their number. - **Our blob store does not corrupt / lose objects:** - Amazon S3 has **99.999999999%** durability. - **Our software is coded perfectly:** - We cannot account for this. Now let's be clear, this is a back-of-the-envelope calculation. Measurement is the only way to be confident in your statistical properties. > Probability of Success = _Clocks are accurate_ and _Servers will not hang for > 1 hour_ and _Database does not corrupt record_ and _Blob store does not corrupt object_ > Probability of Success = 12-_9s_ * 6-_9s_ * 11-_9s_ * 11-_9s_ = **99.99989%** Approximately 5-6 nines of reliability depending on how you round. That means out of every hundred thousand to one million requests, we'd expect one to be corrupted. That's not fantastic, but we've been pessimistic, and it's mostly driven by our unresponsiveness calculation. Let's be a little more realistic. - **Servers will not hang for > 1 hour** - Assume servers have 99.9% uptime. - Assume only 10% of the downtime is due to a hang. - Assume requests will only be accepted during the first 10% of hang downtime. - Assume servers fail independently. - 99.999% chance a request will not hang. - Assume our server monitoring solution has 99.9% uptime. - **99.999999%** chance for any given request that it won't be on an unresponsive server. > Probability of Success = 12-_9s_ * 8-_9s_ * 11-_9s_ * 11-_9s_ = **99.999998%** Approximately 7-8 nines of reliability. That means every 10 - 100 million requests we'd expect one to be corrupted. That feels like an acceptable lower bound of consistency, although we would still hope to do better in practice. ## A brief retrospective That was a long road, but hopefully an interesting one. Some insights: - It was possible to evolve the model with relatively minimal changes from step to step. - It was possible to add a whole actor type to the model **Storage Cleaner** while leaving **Server** states mostly unchanged. - We added detail to the model as needed to implement our solution. - Adding detail to a model is different than adding functionality: you can simply describe what is already present as it becomes relevant. - Creating a model with assumptions helps estimation. - The model checker was able to catch errors that would otherwise have been caught in production. - The details the model checker provided on error were equivalent or superior to what you could get from an observability solution. We can now confidently code this design using the TLA template as a specification. This doesn't alleviate the need for unit, integration, and system tests. However, it does give you guidance as to what is critical to unit and integration test during the development process. It can also provide guidance for inspection and naming. > _Coding it, testing it, and deploying it to production are left as an exercise for the reader._ | [Cache invalidation](../../caching) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/database-blob/storage-cleaner-working/",
    "relUrl": "/database-blob/storage-cleaner-working/"
  },"3": {
    "doc": "(Verifying Correctness) A working solution",
    "title": "(Verifying Correctness) A working solution",
    "content": "# {{page.title}} {: .no_toc } 1. TOC {:toc} ## Designing the working solution The problem we faced in the last solution was that the images were overwriting each other in the blob store. To perform a write, the blob was overwritten, leaving the system in an inconsistent state that would be returned by the read. The solution to this is simple: We generate a UUID for image, store it, and then associate it with the database record. That way the database record and image change together in a transactional manner. ### Write Profile ```plantuml @startuml participant Client participant Server participant Database participant \"Blob Store\" as BlobStore Client -> Server: Submits request Server -> Server: Generates UUID for image Server -> BlobStore: Writes blob, with UUID as key Server Database : Writes record with userId as key \\nRecord contains metadata and\\nUUID of image Server Server: Submits request Server -> Database : Reads database record with userId as key Server BlobStore: Reads blob, with stored UUID as key Server _Note: Deciding on what assumptions to make will be harder when the probability of failure is more likely. For example, a 1/(1 billon) event would happen to Facebook many times a day. It is critical to track your assumptions and back them up with observability/monitoring or other techniques._ ## Modeling the working solution The state machine is unchanged, but the state definitions will be updated. {% include code.html path=\"working\" %} ## Verifying the solution ### Single Server We're pretty confident this solution is going to work, but we've been confident before. We'll start our test using just a single server, **s1**. {% highlight tla %} SPECIFICATION Spec CONSTANTS SERVERS = {s1} METADATAS = {m1, m2} USERIDS = {u1} IMAGES = {i1, i2} UUIDS = {ui1, ui2, ui3, ui4, ui5} CONSTRAINT StopAfter3Operations INVARIANT TypeOk ConsistentReads {% endhighlight %} We also limit our total operations (*Read* or *Write*) to 3. That might not sound like much, but it's caught all our errors so far. But this time, instead of errors, we get back a state space: {% include states.md states=page.singlestate namespace=\"singlestate\" modelcfg=\"working_small.cfg\" %} This is all the states that were tested by the checker. And it worked! We're not done yet, though; this is just a single server. Let's see what happens when we throw a second one in the mix. ### Two Servers We are now testing with two servers, **s1**, and **s2**. We are again stopping after 3 operations. {% highlight tla %} SPECIFICATION Spec CONSTANTS SERVERS = {s1, s2} METADATAS = {m1, m2} USERIDS = {u1} IMAGES = {i1, i2} UUIDS = {ui1, ui2, ui3, ui4, ui5} CONSTRAINT StopAfter3Operations {% endhighlight %} This test passes too! {% include states.md states=page.multistate namespace=\"multistate\" modelcfg=\"working_standard.cfg\"%} Note how the number of states has expanded drastically. This is to be expected; two servers can interact in a lot more ways than one server. So the two tests that have been guiding our entire development have passed! Are we done now? Not quite. ### Final large test Before we can be really confident, we have to run a larger test. We're going with 4 servers this time, and way more types of images and metadata. We're also going up to 10 operations. This is overall a much more representative test. Why didn't we start with it? {% highlight tla %} CONSTANTS SERVERS = {s1, s2, s3} METADATAS = {m1, m2, m3} USERIDS = {u1, u2, u3, u4, u5} IMAGES = {i1, i2, i3} UUIDS = {ui1, ui2} CONSTRAINT StopAfter10Operations {% endhighlight %} The state space grows exponentially on the number of constants. The previous tests completed in seconds. This test completed in hours. It's easy to make a test that will take days. The recommended technique is to start small and work your way up. Finally, do a large test on a powerful machine. [The TLA+ toolbox can even spin up fast cloud workers.](https://nightly.tlapl.us/doc/cloudtlc/) If you get failures with small tests, a large test isn't going to be any better. But if all your small tests are passing, it's time to really stress test the solution. In general, running the biggest test you can afford to wait for is the right answer (unless you know particular details about your system that would make expanding the test unnecessary). Here is the final state space. Look how large it is: {% include states.md states=page.largestate namespace=\"largestate\" modelcfg=\"working_large.cfg\"%} Precise state profiling needed to be turned off just so this model would run. ### Summary Now we can be confident in our solution. And this is how it will remain forever, perfect and untouched! I bet no one is going to want to add features. Oh wait... | Next: [(Adding Requirements) A more cost efficent solution](../cost-efficent) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/database-blob/working/",
    "relUrl": "/database-blob/working/"
  },"4": {
    "doc": "(Start of Process) The naive first draft",
    "title": "(Start of Process) The naive first draft",
    "content": "# {{page.title}} {: .no_toc } 1. TOC {:toc} ## Solution design Let's start by writing the hackathon solution to this problem (but specify it with way too much detail). We will describe what's going on using a [UML Sequence Diagram](https://developer.ibm.com/articles/the-sequence-diagram/). ### Write Profile This represents a client, most likely a website, submitting multi-part form data consisting of a profile image and JSON metadata describing the contents of the profile. ```plantuml @startuml participant Client participant Server participant Database participant \"Blob Store\" as BlobStore Client -> Server: Submits request Server -> Database : Writes metadata with userId as key Server BlobStore: Writes/overwrites blob with userId as key Server Server: Submits request Server -> Database : Reads metadata with userId as key Server BlobStore: Reads blob with userId as key Server Waiting Waiting --> StartWrite StartWrite --> WriteMetadata StartWrite --> FailWrite WriteMetadata --> WriteBlobAndReturn WriteMetadata --> FailWrite WriteBlobAndReturn --> Waiting Waiting --> StartRead StartRead --> ReadMetadata ReadMetadata --> ReadBlobAndReturn ReadBlobAndReturn --> Waiting @enduml ``` Now it's time to model the system formally. I've written the narrative into the comments, so even if you don't know TLA+, please read the comments in order. {% include code.html path=\"naive\" %} It should be noted that ConsistentReads is a relatively weak success criteria. All it says is that anything returned by Read must have been written in one go. It doesn't say how recently it needs to have been written. It doesn't say that reads couldn't go backward in time and regress. Yet this invariant is sufficient to catch all the flaws that will arise in this design. We don't need to model every aspect of the system, only the aspects we are concerned about. In this case it is making sure the blob store is consistent with the database. Common sense may be used to say that the design keeps the storage up to date. ## Logically debugging the solution ### Starting small Remember the CONSTANTS section above? Now it is time to define them. {% highlight tla %} CONSTANTS SERVERS = {s1} METADATAS = {m1, m2} USERIDS = {u1} IMAGES = {i1, i2} {% endhighlight %} As you can see, each of the constants is a set. What really matters is the cardinality (size) of each set. By only having one server and one user, we are testing on a very small version of our problem. It's good to start small, and then expand the tests as your small tests succeed. Note that we have two metadatas and two images. If we only had one, it would be as though there were only a single profile and single image that could be uploaded. That would likely lead to a lot of tests passing that really shouldn't. Determining the size of the simulation is a bit of an art, but if it doesn't work for a single server and a single userId, we know we're in trouble. And oh no! We're in trouble. When you run the model, the error trace shows up in the model checker UI as shown below. Don't bother reading it in detail, because we have a better way of exploring it. For our purposes we will be visualizing the model checker errors as shown below. The writing with a lighter font weight (and right aligned on desktop) are notes describing the failure and part of the narrative. {% include trace.html traceconfig=page.smalltrace constraint=\"Invariant ConsistentReads is violated.\" trace=site.data.database-blob.naive.small modelcfg=\"naive_small.cfg\" %} So we know we have at least one design flaw. If a Write fails after the database has been written to, but before the blob store has been written to, the next reads will be corrupted. This is true even if there is a single server. We will try to address this flaw in the next session. ### Increasing the size of the simulation Normally after a failure, you'd attempt to correct your design before running another simulation. But I'm curious if the system fails faster or in a different way when there are multiple servers. {% highlight tla %} CONSTANTS SERVERS = {s1, s2} METADATAS = {m1, m2} USERIDS = {u1} IMAGES = {i1, i2} {% endhighlight %} {% include trace.html traceconfig=page.multitrace constraint=\"Invariant ConsistentReads is violated.\" trace=site.data.database-blob.naive.multiserver modelcfg=\"naive.cfg\" %} Good thing we ran this simulation. Even in the absence of failure, there is a race condition that can lead to incomplete data being returned. Looks like we have some work to do! | Next: [(Progressive Refinement) An improved solution](../improved) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/database-blob/naive/",
    "relUrl": "/database-blob/naive/"
  },"5": {
    "doc": "(Adding Requirements) A more cost efficent solution",
    "title": "(Adding Requirements) A more cost efficent solution",
    "content": "# {{page.title}} ## The new requirements hit You've hit the big time. Absolute perfection. You've just built the database blob store coordinator [to beat them all](../working). Nothing is going to bring you down... until your boss walks into your office. \"I just talked to Jim in DevOps,\" she says ominously, \"and we're getting hosed on our data storage costs. Apparently this last update writes a new object every time someone updates their profile. Even if they update it again, all the old stuff sits around.\" \"But it's way more correct this way,\" you respond defensively. \"I modeled and verified it. That means I'm right, I win the conversation. Good day sir/madam.\" \"Not so fast, bucko,\" she retorts. \"You know what I always say: 'Time is money, money is money, and cloud storage costs are money.' You're going to have to figure it out.\" \"But please, boss,\" you plaintively mewl, \"I just got this working correctly. I proved it and everything. Running that last test took twelve hours. Who knows what will happen if I go mucking about in there?!\" She looks at you with a mixture of contempt and pity. \"Don't you know one of the best characteristics of formal models is that you can build on existing models, make them more robust, and use them to verify change?\" A realization hits you. You underestimated her technical skills due to her alignment with company priorities and the profit motive. She was absolutely correct: modeling is very useful for evolving designs. Duly chastened, you get back to work, and we go back to using a collective pronoun. \"One more thing\", says our boss with an evil glint in her eye. \"Team Ninja-Dragon is integrating their latest sprint. The **Server** codebase is frozen. You won't be able to add any more functionality on that component.\" ## Updated system components So we have to implement data cleanup without adding functionality to the **Server**. This means we need to create a new microservice, the **Storage Cleaner**. It will need to be able to read from the **Database** and **Blob Store** to find orphaned files, then delete them from the **Blob Store**. It will likely be triggered periodically, perhaps by a cloudwatch alarm; however, at large enough scale it may stay on permanently. The component diagram looks like this: ```plantuml @startuml skinparam fixCircleLabelOverlapping true left to right direction component [Blob Store] as BlobStore () \"Blob Read\" as BlobRead () \"Blob Write\" as BlobWrite [BlobStore] -up- BlobRead [BlobStore] -up- BlobWrite component [Database] () \"Metadata Read\" as DatabaseRead () \"Metadata Write\" as DatabaseWrite [Database] -up- DatabaseRead [Database] -up- DatabaseWrite component [Storage Cleaners] as StorageCleaner () \"Time triggered activation\" as TimeTrigger StorageCleaner -up- TimeTrigger StorageCleaner ..> DatabaseRead : uses StorageCleaner ..> BlobRead: uses StorageCleaner ..> BlobWrite: deletes component [Servers] Servers ..> DatabaseRead : uses Servers ..> DatabaseWrite : uses Servers ..> BlobRead: uses Servers ..> BlobWrite: uses () \"Create or Update Profile (userId)\" as WriteProfile () \"Read Profile (userId)\" as ReadProfile Servers -up- ReadProfile Servers -up- WriteProfile @enduml ``` We have two main design considerations at this point: - We will have to plan for more than one **Storage Cleaner** to be active simultaneously. They could be run in a replica set, or delays could cause triggered instances to overlap. - We will need to ensure that the behavior of **Storage Cleaner** doesn't break the invariants we tested previously. ## A formal definition of success Before starting work, it's good to understand the definition of success. Because of our previous modeling work, we can now state it formally. {% include code.html path=\"success\" %} Ideally, we'd like to never have an orphan file. Let's test that really quick. {% include trace.html traceconfig=page.always constraint=\"AlwaysNoOrphanFiles is violated.\" trace=site.data.database-blob.cost-efficent.always %} Failing on the first write sounds like a bad success criteria. Instead we'll go with **AlwaysEventuallyNoOrphanFiles** as our definition of success. |Next: [(Implementing New Requirements) A naive update](../storage-cleaner-naive) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/database-blob/cost-efficent/",
    "relUrl": "/database-blob/cost-efficent/"
  },"6": {
    "doc": "(Implementing New Requirements) A naive update",
    "title": "(Implementing New Requirements) A naive update",
    "content": "# {{page.title}} {: .no_toc } 1. TOC {:toc} ## Updating the design The **Storage Cleaner** is going to to query the blob store and get a batch of keys. It will then query the database in one query and find all the images that are missing a database entry. Then it will delete those unused keys using a batch API call. ### Storage Cleaner Run ```plantuml @startuml participant \"Storage Cleaner\" as StorageCleaner participant Database participant \"Blob Store\" as BlobStore StorageCleaner -> BlobStore: Gets batch of keys StorageCleaner Database : Queries for unused keys StorageCleaner BlobStore: Batch deletes unused keys StorageCleaner StorageCleaner: Repeats from beginning end @enduml ``` ## Modeling the design This is the first example in our modeling tasks in which the model will not match the solution 1-1. - **Relaxing a constraint**: While the design calls for batches, for simplicity's sake we will model it as if the entire blob store keyset can fit into one batch. This hides the complexity of figuring out which items have already been checked and how large of a batch size to use; we can either handle these considerations in implementation or model them separately. In this model, we will need to handle new keys being added after we query for keys, as well as the deletion process failing before all key are deleted. This should also alert us to problems that may be introduced by batching. _Note: This design decision is a judgment call that may or may not be correct, but it holds for the current examples._ - **Enhancing a constraint**: Deleting from the blob store will be modeled as a one by one operation, even though it is submitted in one API call. This is because blob stores don't provide transactions. A batch delete may happen over the course of time. The **Storage Cleaner** state diagram looks like this: ```plantuml @startuml hide empty description [*] --> Waiting Waiting --> CleanerStartGetBlobKeys CleanerStartGetBlobKeys --> CleanerGetUnusedKeys CleanerStartGetBlobKeys --> CleanerFail CleanerGetUnusedKeys --> CleanerDeletingKeys CleanerGetUnusedKeys --> CleanerFail CleanerDeletingKeys --> CleanerFinished CleanerDeletingKeys --> CleanerFail CleanerFinished --> Waiting @enduml ``` Only the core additions to the spec are shown here. Click _Download Code_ or _Download PDF_ to see the whole thing. {% include code.html path=\"storagecleanernaive\" snippet=\"storagecleanernaive-snippet\" %} ## Verifying the design Let's start small and see what happens: {% highlight tla %} CONSTANTS SERVERS = {s1} CLEANERS = {c1} {% endhighlight %} {% include trace.html traceconfig=page.single constraint=\"Invariant ConsistentReads is violated.\" trace=site.data.database-blob.storage-cleaner-naive.single modelcfg=\"storagecleanernaive_small.cfg\" %} Let's try it again with two servers and two cleaners to see if we get different behavior. {% highlight tla %} CONSTANTS SERVERS = {s1, s2} CLEANERS = {c1, c2} {% endhighlight %} {% include trace.html traceconfig=page.multi constraint=\"Same behavior. Invariant ConsistentReads is violated.\" trace=site.data.database-blob.storage-cleaner-naive.multi modelcfg=\"storagecleanernaive.cfg\" %} Adding more servers and cleaners didn't change the failure mode. We've likely hit upon the essential failure of this design. ### Summary Clearly this solution isn't going to work as is. It can delete images that were part of records being created at that moment. Normal cleanup systems don't do that; normally they wait a little while... | Next: [(Implementing New Requirements) Significant improvement](../storage-cleaner-improved) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/database-blob/storage-cleaner-naive/",
    "relUrl": "/database-blob/storage-cleaner-naive/"
  },"7": {
    "doc": "(Progressive Refinement) An improved solution",
    "title": "(Progressive Refinement) An improved solution",
    "content": "# {{page.title}} {: .no_toc } 1. TOC {:toc} ## Refining the design Okay, so the big problem seems to be that the database is written before the blob store. This allows records to be read before they are ready. It also allows the system to fail in a state in which records that should not be readable are readable. Let's update our design such that: 1. The blob store is written before the database. 2. If the metadata isn't present in the database, the server returns an empty record. This should fix both of the problems we saw before. ### Write Profile ```plantuml @startuml participant Client participant Server participant Database participant \"Blob Store\" as BlobStore Client -> Server: Submits request Server -> BlobStore: Writes/overwrites blob, with userId as key Server Database : Writes metadata with userId as key Server Server: Submits request Server -> Database : Reads metadata with userId as key Server BlobStore: Reads blob, with userId as key Server Waiting Waiting --> StartWrite StartWrite --> WriteBlob StartWrite --> FailWrite WriteBlob --> WriteMetadataAndReturn WriteBlob --> FailWrite WriteMetadataAndReturn --> Waiting Waiting --> StartRead StartRead --> ReadMetadata StartRead --> ReadMetadataAndReturnEmpty ReadMetadataAndReturnEmpty --> Waiting ReadMetadata --> ReadBlobAndReturn ReadBlobAndReturn --> Waiting @enduml ``` Then we update the formal specification. > _Note: Comments have changed to reflect the narrative. See the previous page for more comprehensive comments._ {% include code.html path=\"improved\" %} ## Checking our improved design ### Starting small Let's start with our our single server case again. {% include trace.html traceconfig=page.single constraint=\"Single server still errors. Invariant ConsistentReads is violated.\" trace=site.data.database-blob.improved.single modelcfg=\"improved_small.cfg\" %} We don't have a perfectly working solution yet, but notice that the previous solution failed in 7 steps, whereas this solution failed in 10. Generally, the more steps that need to occur before failure, the more unlikely the failure—but not always. Let's take a closer look at the steps. In this case, the single server needed to write successfully, then write unsuccessfully, then read. In the previous one, the server just needed to write unsuccessfully, then read. But once we spell it out, that doesn't increase our confidence. Of course users are going to write multiple times, over days, so effectively all we need is a write failure for an error to occur. ### Testing multiple servers It may be informative to see how multiple servers fail. {% include trace.html traceconfig=page.multi constraint=\"Invariant ConsistentReads is violated.\" trace=site.data.database-blob.improved.multi modelcfg=\"improved.cfg\" %} Now it fails in 9 steps rather than 6. But again, all it needs is a successful write to happen before the simultaneous read and write. The error hasn't fundamentally changed that much. ### Summary So we've eliminated one class of error: the kind in which the blob store is unset and returned in a read. The problem now is that the blob store and the metadata can get out of sync. This can happen because of either a failure while writing, or one server writing while another one is reading. We're not done yet. But I have a hunch we can make it work. |Next: [(Verifying Correctness) A working solution](../working) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/database-blob/improved/",
    "relUrl": "/database-blob/improved/"
  },"8": {
    "doc": "Coordinating a Database and Blob Store",
    "title": "Coordinating a Database and Blob Store",
    "content": "# {{page.title}} {: .no_toc } 1. TOC {:toc} ## Introduction Let's start with something that millions of engineers do each year: coordinating databases and blob stores (like Amazon S3, Azure Blob, GCP Cloud Storage). Why might we need to do this? You have a site with users. You want each user to have a profile image. How do you do this? The easiest way is to store the user metadata and the image in your database. You can do this transactionally: either the metadata and the image are both stored, or neither is stored. This way you don't have to worry about inconsistent cases where the image is stored, but the user isn't, or the user is stored, but the image isn't. In practice, this approach doesn't scale easily or cheaply. Transactional databases are typically the performance bottleneck in a system, and you don't want to take up their storage, read, and write capacity unnecessarily. What if users need to add more media beyond just a profile picture? This can quickly become an expensive prospect. Blob stores, on the other hand, are relatively cheap and highly scaleable and can easily hold as many files as you'd practically need. They also don't consume database CPU cycles. Just one problem: they aren't tied in with the database's transaction coordinator. You as the programmer need to figure out an application level solution to link the database and blob stores, so when a request comes in for a user and their profile picture, you can return it. This is a simple and common distributed system, which makes it a perfect place to start. ## Modeling the problem We have three components in this system that we must consider: the **Database**, the **Blob Store** and the **Servers**. Note we consider the **Database** and the **Blob Store** each as individual entities. If we were to look at their implementation, we would potentially see shards and replication over a number of instances. However, the designers of those products expose interfaces with particular properties that can hide this complexity. - **Database**: Allow [ACID Transactions](https://databricks.com/glossary/acid-transactions). The important part for us is that the full database update either will succeed or fail; we don't need to account for a partial completion of the update. - **Blob Store**: All the main blob stores have [Read After Write](https://levelup.gitconnected.com/aws-azure-gcp-object-storage-services-5f1b2945cc11) consistency. This means that as soon as a write returns successfully, all subsequent reads will return the new value. This shouldn't be taken for granted in eventually consistent systems; it may take time after a write is completed to be sure that value will be returned in all cases. Additionally, all the blob stores are atomic on writes, meaning we don't have to account for files being partially uploaded and left in an invalid state that clients could read. The **Servers**, however, are our responsibility. They use the read and write interfaces exposed by the data stores however our application is written to use them. Note the plural in Servers: we have to deal with the fact that multiple instances of our application are running simultaneously. We are assuming a single thread per Server, processing a single request at a time. Although in practice many threads could be on a single **Server**, it is logically equivalent to the case where there are multiple **Servers** each with one thread. We can model this with the [UML Component Diagram](https://www.smartdraw.com/component-diagram/) below: ```plantuml @startuml skinparam fixCircleLabelOverlapping true left to right direction component [Blob Store] as BlobStore () \"Blob Read\" as BlobRead () \"Blob Write\" as BlobWrite [BlobStore] -up- BlobRead [BlobStore] -up- BlobWrite component [Database] () \"Metadata Read\" as DatabaseRead () \"Metadata Write\" as DatabaseWrite [Database] -up- DatabaseRead [Database] -up- DatabaseWrite component [Servers] Servers ..> DatabaseRead : uses Servers ..> DatabaseWrite : uses Servers ..> BlobRead: uses Servers ..> BlobWrite: uses () \"Create or Update Profile (userId)\" as WriteProfile () \"Read Profile (userId)\" as ReadProfile Servers -up- ReadProfile Servers -up- WriteProfile @enduml ``` We are implementing two interfaces with our server: *Write Profile* and *Read Profile*. - *Create or Update Profile (userId)*: leads to one of the following post conditions: - The database contains metadata about the profile. The blob store contains the profile image. There is some way to link the two. - The API call fails in such a way that both datastores are left in a consistent state that does not disrupt *Read Profile* or prevent a successful retry. - *Read Profile (userId)*: For a given profile, both the metadata and the profile image are returned. It is unacceptable to return one without the other. Returning None is acceptable when a good result is impossible, but unacceptable otherwise. Note that we are not modelling the clients of these APIs. However, the consistency guarantees set out above would allow someone else to do so. | Next: [(Start of Process) The naive first draft](naive)| ",
    "url": "http://localhost:4000/pragmaticformalmodeling/database-blob/",
    "relUrl": "/database-blob/"
  },"9": {
    "doc": "Working cache invalidation",
    "title": "Working cache invalidation",
    "content": "{% include title_toc.md %} ## Designing a working solution We need to account for inflight cache fill requests. Let's remind ourselves what we know about them: - They are triggered in response to a read request. - They happen synchronously or at least near realtime so a response can be sent back to the client. From a design perspective, then, we can set certain parameters: - No key corresponding to an inflight request will be evicted, as it is needed to respond to an active request. Therefore the key belongs in the cache. - We should process invalidation messages for keys corresponding to inflight requests. We know the key will be added to the cache soon, so it may be relevant. ### Working cache invalidation ```plantuml @startuml participant \"Writer\" as Writer participant \"Database\" as Database participant \"Cache Invalidation Queue\" as Queue participant \"Cache\" as Cache Writer -> Database: Updates data Database -> Queue: Adds updated key and\\nversioned data to queue Queue Cache: Returns invalidation item alt Key not in Cache Cache -> Cache: Does nothing alt Key in cache fill requests Cache -> Cache: Adds key and data from\\ninvalidation message to cache end end alt Key in Cache Cache -> Cache: Does nothing alt Cache data older\\nthan invalidation Cache -> Cache: Replaces old version\\nwith data from message end end @enduml ``` ## Modeling We do have one modeling consideration. We mentioned that keys could be evicted at any time, which accounted for server crashes as well as caching policy. We can model a server crash as a cache fill failure followed by an eviction. Therefore, we can already account for this scenario without needing to extend our current model. {% include code.html path=\"cacheinvalidationv3\" snippet=\"cacheinvalidationv3snippet\" %} ## Verification We run a larger test on this model. {% highlight tla %} SPECIFICATION Spec CONSTANTS KEYS = {k1, k2} INVARIANT TypeOk PROPERTY AlwaysEventuallyDatabaseAndCacheConsistent CONSTRAINT DatabaseRecordsDoNotExceedMaxVersion {% endhighlight %} And it passes: {% include states.md states=page.v3states namespace=\"v3\" modelcfg=\"cacheinvalidationv3.cfg\"%} We trust this result more because we have seen earlier versions of the system fail in rational ways against the same invariants and properties. ## Retrospective There are a couple of key modeling takeaways from this series: - **Implementing another module:** We used the exact same cacherequirements module for the 4 iterations of our caches. Not only does this follow _do not repeat yourself (DRY)_ principals, but it also shows how we can maintain requirement consistency across many implementations. While this was a simple example, it is possible to import much more subtle requirement modules such as \"linearizable\" and map them into your specific module. They can then provide Invariants and Properties that let you know if your solution is working correctly. - **Compactness of specifications**: The entire set of requirements and common data properties was 50 lines long with extensive comments. The final working cache specification was 220 lines long. This modeled and tested reasonably sophisticated cache invalidation logic. This is much less verbose and time-consuming than implementing the logic and tests in a standard programming languages. This is part of why formal modeling is a great next step after whiteboarding or diagramming. The output is small enough to be read, critiqued and tested in one sitting. - **Fairness**: While it can be tempting just to write _WF_vars(Next)_ as part of your specification, you often need to break it up further. If we were to write that, evictions would HAVE to occur, or worse Database updates would HAVE to happen, meaning that we would not necessarily be able to catch temporal property violations. Without fairness, the cache could simply never update, which would break eventual consistency. That circumstance is something we have to prevent in code and/or with monitoring solutions. We can represent that effort as _WF_vars(CacheFairness)_, providing fairness to all operations that MUST eventually occur. So where do we go from here? There are two options. The next (and final) page in this series is about replicating the bug described by the Facebook paper. | Next: [Reproducing Facebook's bug](../reproducing-the-bug) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/caching/working-cache-invalidation/",
    "relUrl": "/caching/working-cache-invalidation/"
  },"10": {
    "doc": "A naive model of caching",
    "title": "A naive model of caching",
    "content": "{% include title_toc.md %} ## Designing a naive cache In addition to the [parameters described previously](../#initial-parameters-for-all-exercises), we will make a few design decisions: - Caches will only be filled during reads (as pictured below). - We will not worry about cache invalidation. - We will only model a single cache. We are assuming it is either one server, or one consistent replica set. _Note: While there are generally many cache servers, the complexity of cache invalidation (at this level) can be modeled with just one cache and one database._ So essentially the only operation we are implementing is the read described in the last section. ```plantuml @startuml participant \"Web Server\" as Server participant \"Cache\" as Cache participant \"Database\" as Database Server -> Cache: Requests query alt cache miss Cache -> Database: Requests query result Database --> Cache: Returns data Cache -> Cache: Caches query result end Cache --> Server: Returns data @enduml ``` Now we have all our design parameters. Let's model! ## Success criteria Because caches are so well understood, we can come up with our success criteria before modeling. Note how it defines certain key data models, so success can be defined. {% include code.html path=\"cacherequirements\" %} > Note: the requirements are their own TLA+ module. In all our models, we will import it. We can think of it a bit like an interface in a standard programming language. ## Modeling the cache We model the cache, importing the cache requirements module rather than redefining the expressions it provides. {% include code.html path=\"naivecache\" %} ## Verifying the cache First let's try setting DatabaseAndCacheConsistent as an invariant and see what happens. This says that the cache and the database must always be consistent with each other. {% include trace.html traceconfig=page.always constraint=\"Invariant DatabaseAndCacheConsistent is violated.\" trace=site.data.caching.naive-model.always %} This is what we'd expect. The **Cache** and the **Database** are not always consistent with each other. If this passed, we should doubt the model. What if we use the eventually consistent property _AlwaysEventuallyDatabaseAndCacheConsistent_? We get another error. Temporal property violations are not as clear as logical ones. {% include trace.html traceconfig=page.eventually constraint=\"Temporal properties were violated.\" trace=site.data.caching.naive-model.eventually %} Because we have no way of guaranteeing that a key will be evicted from the cache, as soon as the key is set, the cache will not change. ## Summary As we can see, our current model of cache is not eventually consistent with the database. We need to systematically clear the cache of outdated values. We need _cache invalidation_. | Next: [Adding cache invalidation](../cache-invalidation) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/caching/naive-model/",
    "relUrl": "/caching/naive-model/"
  },"11": {
    "doc": "Adding cache invalidation",
    "title": "Adding cache invalidation",
    "content": "{% include title_toc.md %} ## Designing an initial cache invalidation solution As we determined earlier, we need to be able to systematically evict out-of-date values from the cache. We do that with cache invalidation. Whenever the database updates a value, it put an invalidation message on a queue. The cache will process messages from that queue: if it contains the key it will evict the value, otherwise it will do nothing. Assumptions: - Our invalidation queue does not guarantee in-order delivery. - Our invalidation queue is durable and guarantees at-least-once delivery. ### Initial cache invalidation ```plantuml @startuml participant \"Writer\" as Writer participant \"Database\" as Database participant \"Cache Invalidation Queue\" as Queue participant \"Cache\" as Cache Writer -> Database: Updates data Database -> Queue: Adds updated key to queue Queue Cache: Returns invalidation item Cache -> Cache: Evicts invalidated key from queue alt Key not in Cache Cache -> Cache: Does nothing end @enduml ``` ## Initial cache invalidation solution ### Modeling We are now dealing with multiple processes, cache fill and invalidation, that may interact. Therefore it is necessary to break the processes down into their component steps, which may be executed simultaneously. Also, for context, a Cache Fill describes the process of the Cache requesting data from the **Database**, the **Database** responding, and the **Cache** incorporating that data. It is now worthwhile to model the cache's state machine. ```plantuml @startuml hide empty description [*] --> Inactive Inactive --> CacheStartReadThroughFill CacheStartReadThroughFill --> DatabaseRespondToCacheFill DatabaseRespondToCacheFill --> CacheCompleteFill DatabaseRespondToCacheFill --> CacheFailFill @enduml ``` There is also a very simple message-handling state machine: ```plantuml @startuml hide empty description [*] --> InvalidationMessageOnQueue InvalidationMessageOnQueue --> CacheHandleInvalidationMessage @enduml ``` Note that the cache requirements and the underlying data models that are checked stay the same. {% include code.html path=\"cacheinvalidationv1\" %} ### Verification When we go to verify it we get an error: {% include trace.html traceconfig=page.v1 constraint=\"Temporal property violated.\" trace=site.data.caching.cache-invalidation.v1 modelcfg=\"cacheinvalidation.cfg\" %} Clearly we have a race condition between cache invalidation and cache fill. Let's try to rectify that. ## Updated cache invalidation solution ### An updated design So our data has been versioned all along for observability. It's time to start using the versions in our solution. This isn't unrealistic, as databases can send snapshot times along with results and invalidations. Let's also start sending the data along with the invalidations, so that we can update the cache when things change. Whenever a cache fill comes back, or an invalidation message is received, we will compare the version we just received to the version in the cache. We will only modify the cache if the version is higher. That way we don't need to be concerned with race conditions of that sort. Whichever comes back first will be compared to the one that comes back second, and the cache will eventually have the same value. #### Updated cache invalidation ```plantuml @startuml participant \"Writer\" as Writer participant \"Database\" as Database participant \"Cache Invalidation Queue\" as Queue participant \"Cache\" as Cache Writer -> Database: Updates data Database -> Queue: Adds updated key and\\nversioned data to queue Queue Cache: Returns invalidation item alt Key not in Cache Cache -> Cache: Does nothing end alt Key in Cache Cache -> Cache: Does nothing alt Cache data older\\nthan invalidation Cache -> Cache: Replaces old version\\nwith data from message end end @enduml ``` ### Modeling We should update the state machines to: ```plantuml @startuml hide empty description [*] --> Inactive Inactive --> CacheStartReadThroughFill CacheStartReadThroughFill --> DatabaseRespondToCacheFill DatabaseRespondToCacheFill --> CacheCompleteFill DatabaseRespondToCacheFill --> CacheFailFill DatabaseRespondToCacheFill --> CacheIgnoreFill : fill version is older than stored version @enduml ``` ```plantuml @startuml hide empty description [*] --> InvalidationMessageOnQueue InvalidationMessageOnQueue --> CacheHandleInvalidationMessage InvalidationMessageOnQueue --> CacheIgnoreInvalidationMessage : message version is older than stored version @enduml ``` This is reflected in the code below. {% include code.html path=\"cacheinvalidationv2\" snippet=\"cacheinvalidationv2snippet\" %} ### Verification We run it and experience a different error. {% include trace.html traceconfig=page.v2 constraint=\"Temporal property violated.\" trace=site.data.caching.cache-invalidation.v2 modelcfg=\"cacheinvalidation.cfg\" %} ## Summary Our main problem remaining is that cache invalidation messages are ignored if the key is not in the cache. In this case, a cache fill can be completed incorrectly with the old value. More broadly, the solution doesn't take ongoing cache fills into consideration. We should address this in our next design. | Next: [Working cache invalidation](../working-cache-invalidation) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/caching/cache-invalidation/",
    "relUrl": "/caching/cache-invalidation/"
  },"12": {
    "doc": "Reproducing Facebook's bug",
    "title": "Reproducing Facebook's bug",
    "content": "{% include title_toc.md %} ## Introduction The [Facebook blog post](https://engineering.fb.com/2022/06/08/core-data/cache-invalidation/) that inspired this series ended with a bug caught by Polaris, Facebook's cache inconsistency observability tool. Like any bug found in production, it would be better if it could be caught in the design process. In this post, we'll try to make a model with sufficient detail to catch this bug. > _This reconstruction is best thought of as historical fiction, the \"what if the British won the Revolutionary War\" version of Facebook's cache behavior. Take it with a bag of salt._ ## Detective work ### The bug report _Taken directly from [this blog post](https://engineering.fb.com/2022/06/08/core-data/cache-invalidation/)._ Precondition: The cache filled metadata. {% highlight docker%} 1. The cache tried to fill the metadata with version. 2. In the first round, the cache first filled the old metadata. 3. Next, a write transaction updated both the metadata table and the version table atomically. 4. In the second round, the cache filled the new version data. Here, the cache fill operation interleaved with the database transaction. It happens very rarely because the racing window is tiny. You might be thinking, “This is the bug.”. No. Actually, so far everything worked as expected because cache invalidation is supposed to bring the cache to a consistent state. 5. Later, cache invalidation came during an attempt to update the cache entry to both the new metadata and the new version. This almost always works, but this time it didn’t. 6. The cache invalidation ran into a rare transient error on the cache host, which triggered the error handling code. 7. The error handler dropped the item in cache. The pseudocode looks like this: drop_cache(key, version); It says drop the item in cache, if its version is less than specified. However, the inconsistent cache item contained the latest version. So this code did nothing, leaving stale metadata in cache indefinitely. This is the bug. We simplified the example quite a bit here. The actual bug has even more intricacy, with database replication and cross region communication involved. The bug gets triggered only when all steps above occur and happen specifically in this sequence. The inconsistency gets triggered very rarely. The bug hides in the error handling code behind interleaving operations and transient errors. {% endhighlight %} ### Deriving Assumptions | Assumption | Evidence |:-------------|:------------------| Database updates metadata and version transactionally | Step 3 | The metadata stored in cache, and the version of that metadata, are filled separately (non-transactionally) | Steps 1 and 4 | The metadata stored in cache, and the version of that metadata are filled with separate database requests | Steps 1 and 4 | The cache fill of metadata or version can fail | Step 5| Cache invalidation messages contain both the version and the metadata | Step 5 | Cache invalidation message processing can fail in a way that is not automatically resumed | Step 6 | When cache invalidation fails, an error handler is called that conditionally drops keys lower than the version on the message | Step 7 | Additionally, we can gather information from other sources. | Assumption | Evidence |:-------------|:------------------| The cache invalidation messages are guaranteed to be eventually delivered | [TAO](https://www.usenix.org/system/files/conference/atc13/atc13-bronson.pdf) design paper The main question it raised: How exactly does Facebook use versions in the cache, if they are allowed to get out of sync? The answer seems to be: leaning heavily on their cache invalidation solution (Steps 4 and 5). Because the cache will get every invalidation message, no matter how far the version and metadata are out of sync, the LATEST cache invalidation message should fix it. From that we must infer: _Cache invalidation replaces metadata values of at most equal or lesser version._ The fundamental bug is therefore the fact that the error handler that handles cache invalidation errors drops only **lesser versions**, while the cache invalidation contract requires replacing **equal or lesser versions**. Let's see if we can catch this with a model. ## The model All the descriptions and assumptions are done at a simplified level, as in the blog post. Therefore we will model this level of detail. _Note: Simplification in of itself does not make a model invalid or even less helpful. Models should be constructed to catch bugs at a particular level of abstraction. It is possible that a system may need multiple models, at different levels of abstraction, to describe it. Sometimes you can even connect them together to find bugs in their interactions, though this is very advanced._ As we create a model in an attempt to recreate a system, our biggest risk is [overfitting](https://en.wikipedia.org/wiki/Overfitting)—that is, tailoring our implementation too exactly to get our expected result. We can mitigate this somewhat with Occam's Razor: trying to find the simplest model that fits the assumptions The other assumption we make is that the reported bug is the only bug in Facebook's logic at this level of abstraction. Where data is not available from Facebook's caching papers, we may substitute a fix from the prior articles in this series. We get state machines that look like this: ```plantuml @startuml hide empty description [*] --> Inactive Inactive --> CacheStartFillMetadata CacheStartFillMetadata --> DatabaseRespondWithMetadata DatabaseRespondWithMetadata --> CacheFillMetadata DatabaseRespondWithMetadata --> CacheFailFill Inactive --> CacheStartFillVersion CacheStartFillVersion --> DatabaseRespondWithVersion DatabaseRespondWithVersion --> CacheFillVersion DatabaseRespondWithVersion --> CacheIgnoreFillVersion : Fill version is less than stored version DatabaseRespondWithVersion --> CacheFailFill CacheFailFill --> Inactive @enduml ``` ```plantuml @startuml hide empty description [*] --> InvalidationMessageOnQueue InvalidationMessageOnQueue --> UpdateFromInvalidationMessage InvalidationMessageOnQueue --> IgnoreInvalidationMessage InvalidationMessageOnQueue --> FailInvalidationMessageProcessing: An error occurs FailInvalidationMessageProcessing --> FailUpdateInvalidationMessageIgnore: Version in message less than\\n or equal to version in cache FailInvalidationMessageProcessing --> FailUpdateInvalidationMessageEvictKey: Version in message greater\\n than version in cache @enduml ``` One thing thing to notice in the model is that we import the same _cacherequirements_ we have used for the entire series. They should be sufficient to help us find the bug. {% include code.html path=\"facebookcacheinvalidation\" %} Note how the entire Facebook model fits in 320 lines of code. Also, note how similar it looks to the models we've been using this whole series. It feels like the whole model is at the same level of abstraction; we haven't had to disproportionately model certain pieces just to model our assumptions. ## Can we find the bug? And when we run it we get a bug report that we can actually map, item by item, to the bug reported by Facebook. {% include trace.html traceconfig=page.bug constraint=\"Temporal properties were violated.\" trace=site.data.caching.reproducing-the-bug.bug modelcfg=\"facebookcacheinvalidation.cfg\" %} This trace (or some version of it) is the first thing that comes up when you run the model above. ## Retrospective There are several key takeaways: - **Implementing another module:** Again we used the exact same cacherequirements module for all iterations of our caches. This shows the power of writing requirements in TLA+ and having other modules implement them. - **Compactness of specifications**: The entire set of requirements and implementation was 370 lines of code with plentiful comments. The detail and level of abstraction remained relatively consistent, supporting the representativeness of the spec. - **Potential to catch bugs before production**: The specification is of sufficient detail that the coder of the error handler could have ensured (and even unit tested) conformance to the spec. While this is obviously a simplified model of the actual behavior, there are two potential conclusions to draw: - Modeling at this level of abstraction is useful, and other verification methods could be used to account for the additional complexity such as regions, network partitions, database replication, etc. - The model could be enhanced to a higher level of detail that reflects those complexities, modularized to allow for testing at different levels of abstraction, and then holistically tested (likely over days on a high end machine). > _A final reminder that this is still historical fiction, and likely deviates from the Facebook implementation (even at this level of abstraction) in a number of ways._ | Next: [Time for some Business Logic](../../business-logic) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/caching/reproducing-the-bug/",
    "relUrl": "/caching/reproducing-the-bug/"
  },"13": {
    "doc": "Cache Invalidation",
    "title": "Cache Invalidation",
    "content": "{% include title_toc.md %} ## Introduction Caching is everywhere. Handling caches is one of the more complex and error prone parts of a developer's life, especially when maintaining web applications. Here we will attempt to explain why, and create and debug formal models for caching strategies. Why is the title of this article \"Cache Invalidation\"? As it turns out, that's the tricky part. Don't worry if that doesn't ring a bell yet! We'll explain it step by step. This article was inspired by this [Facebook blog post](https://engineering.fb.com/2022/06/08/core-data/cache-invalidation/) on cache invalidation. As they mention, caching behavior is best thought of as a state machine, which makes it a perfect fit for modeling. I'd recommend you read the post twice: once before and once after you read this article. It is a short and well-written intro to the nuances of caching and cache consistency testing. It's also written by people who run caches that serve one quadrillion queries a day. One quadrillion is **1,000,000,000,000,000**. Statistically speaking, if a problem can happen, it will happen to them. _Note: We'll be focusing on cache reads for this article. However, we will absolutely address what happens when the underlying data is written to (which is where cache invalidation comes up)._ ## A simple explainer on caching The simplest model of data access involves a **Data Consumer** and a **Data Store**. When the consumer wants information, it asks the store, which gives it the most up to data information it has. How simple is that? ### Data Access ```plantuml @startuml participant \"Data Consumer\" as DataUser participant \"Data Store\" as DataStore DataUser -> DataStore: Requests piece of data DataStore --> DataUser: Returns data @enduml ``` ### Cached Data Access For a number of reasons, especially if the data is going to be accessed often, we put a cache between the data consumer and the data store. ```plantuml @startuml participant \"Data Consumer\" as DataUser participant \"Cache\" as Cache participant \"Data Store\" as DataStore DataUser -> Cache: Requests piece of data alt cache doesn't have data (cache miss) Cache -> DataStore: Requests piece of data DataStore --> Cache: Returns data end Cache --> DataUser: Returns data @enduml ``` Wow, that's way more complicated! Spoiler alert, there are way more failures that can occur due to that complexity. So why do we use it? Did Big Cache lobby Congress? Actually, there are profound benefits to caching. The main ones are higher speed and lower cost. Let's look at different examples of caches. ### CPU Caches ```plantuml @startuml participant \"CPU Core\" as Core participant \"L1-L3 Caches\" as Cache participant \"Memory\" as Memory Core -> Cache: Requests data from memory address alt cache miss Cache -> Memory: Requests data from memory address Memory --> Cache: Returns data end Cache --> Core: Returns data @enduml ``` > _Note: L1-L3 caches are grouped in the diagram. Actual data access goes from Core -> L1 -> L2 -> L3 -> Memory_ Perhaps the most important caches in all of computer engineering are those located on the CPU and used to access **Memory**. The caches are located on the CPU, with L1 and L2 caches generally located on each individual core. The closer a cache is to a core, the faster it is to access: [(credit)](https://superuser.com/a/507477) Advantages: - Higher speed: Getting data from an L1 cache is approximately 100x faster than getting it from memory. That matters a lot for data that is actively worked on by the processor. - Lower cost: Caching reduces the amount of times the core needs to access memory. However, lowering cost is not the primary reason for this kind of cache. CPU caches are an example of Read Through caches because the CPU talks to the cache that talks to the memory. They are also [Write Through](https://www.geeksforgeeks.org/write-through-and-write-back-in-cache/) caches, though that is not covered here. If you are interested in this kind of caching, check out [Specifying Systems](https://lamport.azurewebsites.net/tla/book.html?back-link=learning.html#book): **Pages 56-64**. It also addresses writing, which is the more complicated use case. ### Browser Cache ```plantuml @startuml participant \"Web Page\" as WebPage participant \"Browser\" as Browser participant \"Browser Cache\" as Cache participant \"Web Server\" as Server WebPage -> Browser: Accesses image.jpg Browser -> Cache: Checks if image.jpg in cache Cache --> Browser: Doesn't find in cache Browser -> Server: Requests image.jpg Server --> Browser: Returns image.jpg Browser --> WebPage: Returns image.jpg WebPage -> Browser: Accesses image.jpg Browser -> Cache: Checks if image.jpg in cache Cache --> Browser: Returns image.jpg Browser --> WebPage: Returns image.jpg @enduml ``` Browser caches are the type of cache with which your average person has the most interaction. Web browsers such as Chrome maintain a cache of assets they download from the web. Advantages: - Higher speed: Don't need to download large files every time you visit a website or change pages. - Lower cost: Reduces the load on the servers because they don't need to repeatedly serve the largest, and thus most expensive, files. Browser caching is an example of a [Cache-aside](https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside) pattern, where the application is responsible for managing the cache. >_While this is critical to the modern web, it can cause strange problems if it is used even slightly incorrectly. There's a reason the first suggestion when a website stops working is to clear your cache._ ### Database / Data Service Caching This is the type of cache we'll be focusing on in this article. Databases are expensive and hard to scale, and they are also generally a performance bottleneck. So you put a cache in between your web servers and your database. You see both Cache-Aside and Read Through caching here. #### Cache-Aside This is the most common. We are trying to cache database queries, and there is no \"right\" way to do that. For a simple example, let's say we wanted to get the names of all employees located in Palo Alto. The query might look like: > Query.SQL: **SELECT Names FROM EMPLOYEES WHERE Location='Palo Alto'** Ok, that's probably not going to meaningfully change on a second by second basis, so there's no need to keep hitting the database. Our system looks like this: ```plantuml @startuml skinparam fixCircleLabelOverlapping true left to right direction component [Web Server] as Server database [Memcached] as Cache database Database () \"Database Connection\" as DatabaseConnection () \"Read/Write Cache\" as CacheConnection Cache -up- CacheConnection Database -up- DatabaseConnection Server --> CacheConnection Server --> DatabaseConnection @enduml ``` The database is a standard relational database. Memcached is a key value store. We use the entire query string as the key, and the result of the query as the value. ```plantuml @startuml participant \"Web Server\" as Server participant \"Memcached\" as Cache participant \"Database\" Server -> Cache: Checks if contents of Query.SQL is a key in cache Cache --> Server: Doesn't find in cache Server -> Database: Runs Query.SQL on database Database --> Server: Returns the query result Server -> Cache: Writes the pair > Server -> Cache: Checks if contents of Query.SQL is a key in cache Cache --> Server: Returns query result @enduml ``` While potentially you could program a cache like this one to be Read Through, maybe using the query string as a key, it could be a really bad design decision for your workload. Maybe it's better to break it up. This is an application level decision. #### Read-Through If you have more homogenous ways of accessing your data, a read through cache becomes possible. The biggest example of this is the [TAO](https://www.usenix.org/system/files/conference/atc13/atc13-bronson.pdf) graph cache developed by Facebook. While the query language for TAO is not [GraphQL](https://graphql.org/), it's probably not a bad way to think of it. A more semantic, structured lookup language allows you to make company-wide decisions on how to cache, rather than just application-level. To steal an example from the paper, if you wanted to know the number of location checkins at the Golden Gate Bridge, the query might look like this: > **assoc count(534, CHECKIN)** Facebook's system looks like this: ```plantuml @startuml skinparam fixCircleLabelOverlapping true left to right direction component [Web Servers] as Server database [Tao Cache] as Cache database [Database(s) and Data Stores] as Databases () \"Connections\" as DatabaseConnection () \"Object API\" as ObjectAPI () \"Association API\" as AssociationAPI Cache -up- AssociationAPI Cache -up- ObjectAPI Databases -up- DatabaseConnection Cache --> DatabaseConnection Server --> ObjectAPI Server --> AssociationAPI @enduml ``` The TAO cache is the only actor the web servers talk to. It caches the data required to answer the query and returns it to the server. Because it understands the semantics of the query, it might be able to answer more with the data it just cached, like: \"Has Debra been to the golden gate bridge?\" ```plantuml @startuml participant \"Web Server\" as Server participant \"Tao Cache\" as Cache participant \"Databases\" as Databases Server -> Cache: Requests \"query assoc count(534, CHECKIN)\" alt cache miss Cache -> Databases: Requests all needed data from all applicable databases Databases --> Cache: Returns data Cache -> Cache: Intelligently caches graph data end Cache --> Server: Returns data @enduml ``` >_Read Through caching is much better than Cache-aside if you can use it. It's also cleaner to model and analyze, so we'll be using it for the following models. In addition, Facebook allows Write-Through with the Tao cache, which, is out of the scope of this article to preserve the reader's sanity._ #### Advantages of Database Caching Why is this type of caching so important to backend engineers? - **Higher performance**: A database query can take 3ms to 5 seconds depending on its structure. A memcached query can take Facebook handles _1e15 queries per day_. There are _86,400 seconds per day_. _We can round that to 1e10 queries per second_. > A 3 node postgres cluster, tuned for performance, can perform _5,000 queries per second_. Let's call it _2,000 queries per second per node_. We're going to ignore the difficulties of scaling databases and assume we have REALLY good sharding. If we were serving everything from the database, we would need about 5 million servers. If we say each node costs $2,000 per year to run, that's *$10 billion per year*. > A memcached node can handle approx _200,000 queries per second_. We would need a much more manageable 50,000 servers. At $2,000 per node a year, that comes out to the positively bargain price of *$100 million dollars per year*. Caching may be a hard problem, but it is absolutely critical to making SaaS products cost-effective. Your company may not be playing for Facebook-level stakes, but it can absolutely make a difference once a reasonable level of scale has been reached. _[xkcd](https://xkcd.com) gets it:_ ![xkcd on caching](xkcd_the_cloud.png) ## Key concepts and assumptions in database caching > _Note: Some of this can apply to other kinds of caches, but extrapolate at your own risk._ While we covered the read case in some detail, there are several other considerations that will need to make it into every model. ### Modeling assumptions We will be assuming a Read-Through cache for the following models. While this is not the most common type of cache for this purpose, all the cache consistency issues that apply to Read-Through caches also apply to Cache-aside caches. This choice lets us skip past the application-level decisions made by Cache-aside caches. With that said, this article may still help you if you want to model a Cache-aside cache for your specific application. ### Cache eviction Ok, I cached an item. Does it live in my cache forever? No, and there are two main reasons for this: - Your cache is full: Imagine a 10GB cache fronting a 1TB database. At any one time 1/100th of the data can be cached for quick access. What happens when you need to cache something new? It depends on the caching strategy, but [Least-Recently-Used (LRU)](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)) is a standard one. When you need to remove data to add new data, remove the data that has been least recently read. Generally, that will let more relevant data stay in the cache, while less relevant data is evicted. This is important even for giant cache systems spanning thousands of machines. Maybe the North American cache can hold 95% of all the data North Americans want to see, but that 95% keeps changing, so the cache must change with it. - Time to Live (TTL): One simple way to keep your cache up to date is to set a TTL policy. Imagine you never wanted your data to be more than 1 hour out of date. Set a TTL of 1 hour, and after an hour, the data will be retrieved again. This is a blunt but powerful tool. - Accidental Eviction: Caches are generally not built for durability. A cache server can fail, potentially removing the keys it held from the cache. Replication can help with this, but in general caches choose to give up durability for performance. Durability is what the database is for. In general, we should assume a cached item could be evicted at any time for any reason. That doesn't mean we can't model specific reasons for eviction, but we also need to account for random eviction. ### Cache invalidation As convenient as it would be if people only read from databases, it turns out that people write to them too. Which means the database values change. Which means the cache can become out of date. If the cache needed to be immediately consistent with the database it would effectively need to be an extension of the database, with all the accompanying complexity and [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem) scaling difficulties. Databases do this, but it is hidden behind the standard database interface, and not what we're discussing right now. The standard database caches and caching strategies aim to be [eventually consistent](https://en.wikipedia.org/wiki/Eventual_consistency), meaning that given time, the data in the cache will reflect the data in the database (unless of course the database changes again). Theoretically, eventual consistency means your cache could be minutes, hours, or even days behind the database as long as it would eventually catch up. In practice, seconds to minutes are considered acceptable. However, setting a very short TTL reduces the benefit of caching. Some applications can work with a longer TTL if updates that depend on old versions of the data are detected and cause a user error and a cache refresh. For example, choosing a seat on a air flight might show a out of date seat map, but an attempt to book seat that is filled will be caught and the user will be asked to rebook. In many systems, though, this is not a desirable property. Twitter wants to show you the latest Tweets. For that reason, automatic cache invalidation systems exist. Generally, one or more applications read the database logs and look for changes. The application then broadcasts to the caches that a change has occurred and they should evict their old value. The next time someone queries for that data, the cache will read through to the database and get the latest value. ## Initial parameters for all exercises We will work with the simple Read-Through cache below: ```plantuml @startuml skinparam fixCircleLabelOverlapping true left to right direction component [Web Servers] as Server database [Cache] as Cache database [Database] as Databases () \"Connection\" as DatabaseConnection () \"Query API\" as QueryAPI Cache -up- QueryAPI Databases -up- DatabaseConnection Cache --> DatabaseConnection Server --> QueryAPI @enduml ``` This will be how reads are executed for all the exercises, as this behavior is inherent to a Read-Through cache: ```plantuml @startuml participant \"Web Server\" as Server participant \"Cache\" as Cache participant \"Database\" as Database Server -> Cache: Requests query alt cache miss Cache -> Database: Requests query result Database --> Cache: Returns data Cache -> Cache: Caches query result end Cache --> Server: Returns data @enduml ``` Finally, our eviction strategy will be modeled by cache keys randomly being evicted. This covers both the cache running out of space and the cache failing for other reasons. As our primary concern is cache consistency with the database, and not cache availability, it's not necessary to model capacity or more complex eviction strategies. _Note: If we were testing cache eviction strategies, modeling capacity would be essential._ ## Summary Now that we have our caching basics covered, let's model them, test them, and handle the problems that arise! As usual, we will start simple and work our way up. | Next: [A naive model of caching](naive-model) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/caching/",
    "relUrl": "/caching/"
  },"14": {
    "doc": "Learning Material",
    "title": "Learning Material",
    "content": "# {{page.title}} ## Getting Started The best way to start learning TLA+ is with the [TLA+ Video Course](https://lamport.azurewebsites.net/video/videos.html), which is actually very engaging. Then read the first 80 pages of the [Specifying Systems](https://lamport.azurewebsites.net/tla/book.html?back-link=learning.html#book) book, which is somewhat less engaging, but very helpful. The official repository of [TLA+ learning material can be found here](https://lamport.azurewebsites.net/tla/learning.html). > _Note: there's also a language that compiles into TLA+ called PlusCal. It is slightly less powerful (and in fact you may need to use TLA+ expressions inline); however, it looks more like a programming language.[Practical TLA+](https://link.springer.com/book/10.1007/978-1-4842-3829-5) is a good book if you want to learn it. I'm sure it's sufficiently powerful to work the examples on this site. AWS uses both TLA+ and PlusCal for different algorithms. However, as PlusCal compiles to TLA+, understanding TLA+ is generally considered a good idea, regardless of which one you use._ The [TLA+ Language Manual for Engineers](https://apalache.informal.systems/docs/lang/index.html) provides a semi-comprehensive guide to the TLA+ language. The [Learn TLA Website](https://learntla.com/introduction/) has both [TLA+](https://learntla.com/tla/) and PlusCal information. It is less comprehensive, but somewhat more accessible, than the [TLA+ Language Manual for Engineers](https://apalache.informal.systems/docs/lang/index.html). ## Advanced Concepts ### Want structured learning? - [Specifying Systems](https://lamport.azurewebsites.net/tla/book.html?back-link=learning.html#book): **Pages 81-227**: Advanced but well-explained examples - [Weeks of Debugging Can Save You Hours of TLA+ (Video)](https://www.youtube.com/watch?v=wjsI0lTSjIo) - [Blocking Queue](https://github.com/lemmy/BlockingQueue) - [Using TLA+ in the Real World to Understand a Glibc Bug](https://probablydance.com/2020/10/31/using-tla-in-the-real-world-to-understand-a-glibc-bug/) ### Know what you're looking for? - [TLA+ Examples Repository](https://github.com/tlaplus/Examples): Highly advise you read each algorithm along with a paper or explainer, to correlate concepts to code. - [Advanced Concepts from TLA+ Website](https://lamport.azurewebsites.net/tla/advanced.html): Provide references for very specific parts of TLA+. - [Specifying Systems](https://lamport.azurewebsites.net/tla/book.html?back-link=learning.html#book): **Pages 228-End**. Basically a reference manual. ",
    "url": "http://localhost:4000/pragmaticformalmodeling/learning-material/",
    "relUrl": "/learning-material/"
  },"15": {
    "doc": "Enterprise Architect gets us started",
    "title": "Enterprise Architect gets us started",
    "content": "{% include title_toc.md %} ## Introduction ### Your bio You're the kind of person who uses acronyms like [OLTP](https://en.wikipedia.org/wiki/Online_transaction_processing), [OLAP](https://en.wikipedia.org/wiki/Online_analytical_processing) and [OAuth](https://oauth.net/) in a sentence and knows what they mean. Whenever your company gets together for a game of Whiteboard, you win hands down. It's been a little while since you've coded anything, but it's like riding a bike: terrifying and dangerous. Modeling languages are fair game though! ### Your assignment 1. Clarify the customers requirements into a requirements document 2. Distill the requirements document down into a formal specification 3. Come up with an architecture based on the requirements By the end of this, coding the solution should be a piece of cake. > Note: The client has only contracted us to build the backend software and expose APIs. A different firm is handling the frontend. ## Creating the requirements document Through conversations with the customers, you have distilled their requirements into a standard format. ### Functional requirements {% include requirements.md requirements=page.functionalRequirements %} ### Non-Functional requirements {% include requirements.md requirements=page.nonfunctionalRequirements %} ## Requirements summary You feel confident that you've gathered all the necessary requirements to ensure successful delivery on the contract. There are 44 requirements including sub-clauses, which is not that many, but you're worried some might be forgotten due to the tight deliverable schedule. Formal modeling seems like a good way to ensure that doesn't happen, but first you need to come up with an architecture. ## Architecture Based on the requirements, it's safe to chose a simple architecture, with an autoscaling group of **Business Logic Servers** that access a **Database** hosted in a replica set. There is also an external **Payment Processor** that can accept bills, and occasionally provide _Payment Failed callbacks_. ```plantuml @startuml skinparam fixCircleLabelOverlapping true left to right direction component [Payment Processor] as PaymentProcessor database [Database] () \"Database Connection\" as DatabaseConnection component [Business Logic Servers] as Servers component [Clients] () \"Payment Failed(user)\" as PaymentFailed () \"Bill(user, amount)\" as Bill () \"Start Subscription(user)\" as StartSubscription () \"Cancel Subscription(user)\" as CancelSubscription () \"Start Trial(user)\" as StartTrial () \"Cancel Trial(user)\" as CancelTrial () \"Watch Video(user)\" as WatchVideo PaymentProcessor -up- Bill PaymentProcessor -up- PaymentFailed Clients --> WatchVideo Clients --> CancelTrial Clients --> StartTrial Clients --> CancelSubscription Clients --> StartSubscription Servers -down-> PaymentFailed Servers -down-> Bill Servers -up- WatchVideo Servers -up- CancelTrial Servers -up- StartTrial Servers -up- CancelSubscription Servers -up- StartSubscription [Database] -up- DatabaseConnection Servers ..> DatabaseConnection : uses @enduml ``` Now that you've come up with the architecture, you're satisfied that it will work within the requirements and your team won't have any trouble with it. ## Creating the Formal Specification Model Looking over the requirements, it's clear that only the Functional Requirements can be modeled. That might not always be true, but in this case they only influence implementation details and not the Business Logic itself. You don't want to provide the implementation details; after all, that's not your job. You need to write the spec in an implementation-agnostic way. ### The state machine You create a state machine of the user workflow. The state machine here will not necessarily translate directly to a model, but it's still useful to have. ```plantuml @startuml hide empty description [*] --> NotSubscribed NotSubscribed --> SubscriptionStarted NotSubscribed --> TrialStarted: Only if not subscribed or had trial TrialStarted --> SubscriptionStarted TrialStarted --> MonthElapsed MonthElapsed --> SubscriptionStarted TrialStarted --> TrialCanceled TrialCanceled --> NotSubscribed SubscriptionStarted --> SubscriptionCanceled SubscriptionCanceled --> NotSubscribed SubscriptionStarted --> PaymentFailed PaymentFailed --> NotSubscribed @enduml ``` ### Data Model The data model is shared between the specification and all the implementations. It lays out all the event types used for observability. {% include code.html path=\"specdatamodels\"%} ### Specification You trace every requirement above to the specification. This is to ensure that every requirement is either represented formally or deliberately excluded. {% include code.html path=\"speccannonical\"%} ### Stubs You write initial stubs for all the API calls such that you can add appropriate observability. You also provide the basic architecture of the spec to keep the assignment bounded. {% include code.html path=\"stubs\"%} _Note: Generally you expect the first formal spec you write won't be perfect. If failures are being reported when they shouldn't, the spec may need to be revised. There's a collaborative process as the spec gets refined and becomes accurate. We've removed that from this narrative for simplicity. Assume there was a back-and-forth and you're seeing the 3rd revision of the spec above._ ## Summary Satisfied that you have specified and architected the system about as well as you can, you pass it off to a junior developer to implement. You work for a contracting firm. How else are you supposed to pay the bills? | Next: [Junior Developer tries their best](../junior-dev) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/business-logic/enterprise-architect/",
    "relUrl": "/business-logic/enterprise-architect/"
  },"16": {
    "doc": "Principal Engineer saves the day",
    "title": "Principal Engineer saves the day",
    "content": "{% include title_toc.md %} ## Your bio At the age of three you killed Python and wore it as a hat. You have a resume a mile long from all your successfully completed projects, and a thousand yard stare from the ones you couldn't save. You'll probably be a programmer forever, if only because semi-pro competitive tap dance just doesn't pay the bills. ## The assignment 1. Simplify the junior developer's solution. 2. Use the model checker to verify that your elegant solution is, as Einstein would say, \"as simple as possible but no simpler.\" ## Modeling the solution Using the model checker as a guide, you refactor the solution to denormalize the data model somewhat. Whenever possible, you convert logic to simple database transactions. {% include code.html path=\"specprincipal\" snippet=\"principal\"%} You use the model checker periodically to ensure that the simplified design still hit requirements. Once you're confident the design is sufficiently simple, you run a final test on your solution. ## Verifying the solution It passes. {% include states.md states=page.states namespace=\"principal\" modelcfg=\"specprincipal.cfg\"%} Time to start building! ## Design and its effect on automated testing When testing requirements, the standard approach is to map each requirement to one or more specific programmatic tests. For straightforward requirements with immediate triggers and effects, this is not a problem. An integration test can trigger the relevant action and observe the effect. For more complex requirements, those that are more woven into the design, this is challenging. Generally, you trace those requirements to a design document which describes how they are to be fulfilled. Senior engineers and/or architects sign off that the design meets the requirements. Then tests are planned to show conformance with the design. Formal modeling helps us with this process in two ways: - The requirements can be explicitly mapped to a model - A specific design model can be tested against the requirement model The requirements can therefore be straightforwardly verified to be implemented by the design model. It's far easier to test for sub-system and component compliance with the design model than with textual requirements. Automated requirement testing can often turn into glorified regression tests if a team is not careful. By testing to the design model instead, a team can ensure that critical system characteristics are maintained without over-specifying behavior. ## Retrospective Key takeaways from this series: - Precise requirements documents can be modeled formally. - Sloppy logic and design are much more apparent in modeled designs than in code. - Modeled specifications allow you to refactor business logic confidently. - Modeling business requirements means that you can focus unit and integration testing on testing critical behavior. | Next: [You made it to the end! Time to learn TLA+](../../learning-material) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/business-logic/principal-eng/",
    "relUrl": "/business-logic/principal-eng/"
  },"17": {
    "doc": "Junior Developer tries their best",
    "title": "Junior Developer tries their best",
    "content": "{% include title_toc.md %} ## Introduction ### Your bio You recently graduated from a good computer science program. Not only do you have a firm grasp on CS fundamentals, you've also taken electives in operating system and compiler design. None of that has been much help here, though. You keep waiting for someone to ask you to traverse a tree or write a new lexer for C, but you're starting to fear that day will never come. ### Your assignment 1. Try to create a solution that implements the Enterprise Architect's specification. 2. Use the model checker to get your solution working. ## The first attempt ### Modeling _Note: We are assuming the junior programmer is fully competent in TLA+ and code style. Only the architectural and/or requirements knowledge will be lacking._ You start with a simple and clean solution that meets the requirements as you understood them: {% include code.html path=\"specjuniorv1\" snippet=\"juniorv1snippet\"%} ### Verification Your solution does not hold up under test. {% include trace.html traceconfig=page.v1 constraint=\"Invariant StartSubscriptionAccessControl is violated.\" trace=site.data.business-logic.junior-dev.v1 modelcfg=\"specjuniorv1.cfg\"%} There are basic logical errors that need to be fixed. ## The second attempt ### Modeling For this next attempt you work through all the standard logical errors around access control. {% include code.html path=\"specjuniorv2\" snippet=\"juniorv2snippet\" %} ### Verification You test again, running into a billing issue. {% include trace.html traceconfig=page.v2 constraint=\"Invariant SubscribedUsersBilledStartOfMonth is violated.\" trace=site.data.business-logic.junior-dev.v2 modelcfg=\"specjuniorv2.cfg\" %} This is a much more complex business logic error, and it's of the sort that conventional testing may not catch. ## The working attempt ### Modeling In the final attempt there you add significantly more billing logic. {% include code.html path=\"specjuniorv3\" snippet=\"juniorv3snippet\" modelcfg=\"specjuniorv3.cfg\"%} The code is starting to look like a nightmare, but hopefully it will work. ### Verification Testing it leads to success! {% include states.md states=page.v3 namespace=\"junior\" modelcfg=\"specjuniorv3.cfg\"%} ## Next steps So you've gotten a solution working, but frankly, it looks like it'll be a nightmare to implement. Nested if statements all over the place. The database schema is sloppy. It's time to ask for help. > _Note: A valid criticism of the above would be that no one would model business logic like that. And it's true, no one would model something that badly. But people push solutions much worse than this one to production every day. Maybe if they took the time to model, they wouldn't._ | Next: [ Principal Engineer saves the day](../principal-eng) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/business-logic/junior-dev/",
    "relUrl": "/business-logic/junior-dev/"
  },"18": {
    "doc": "Business Logic",
    "title": "Business Logic",
    "content": "{% include title_toc.md %} ## Introduction We've previously used TLA+ mostly for distributed systems problems, but you can also used it to model business logic. First we'll go from requirements to a mathematical specification. Then we'll show two implementations of the specification that lead to the satisfaction of the scenario requirements. Those implementations will be the model for the business logic code to be developed. We'll demonstrate how formal specifications can be used to perform tests of implementations, as well as high confidence refactors. ## The scenario A software design/consulting firm has been contracted to build a website that allows users to watch instructional workout videos online. Let's say it's called MuscleMovies.com. It was founded by former gym and magazine executives, so they have lots of ideas on how to maximize their profits _(or gains, if you will)_. We're talking trial memberships that convert into paid memberships, payment processing and cancellation penalties! In the following pages, we'll get to be three different people: - [The Enterprise Architect](enterprise-architect): who distills the requirements into standard form, makes the architectural choices and writes the formal interface specification. - [The Junior Developer](junior-dev): who is dropped into this project and struggles through the requirements piece by piece. - [The Principal Engineer](principal-eng): who jumps in and saves the day by reimplementing the requirements in a cleaner fashion. It's basically Rashomon, but presented clearly and in chronological order. And with higher stakes, at least according to the MuscleMovies.com CEO. > _Don't worry: despite the tone, we're going rigorous with requirements._ | Next: [Enterprise Architect gets us started](enterprise-architect) | ",
    "url": "http://localhost:4000/pragmaticformalmodeling/business-logic/",
    "relUrl": "/business-logic/"
  },"19": {
    "doc": "Tools and Additional Citations",
    "title": "Tools and Additional Citations",
    "content": "# {{page.title}} ## Tools for TLA+ - [TLA+ Toolbox](https://lamport.azurewebsites.net/tla/toolbox.html): The fully featured (if somewhat outdated) IDE for TLA+. Includes modeling, document generation, and live syntax checking. - [VSCode TLA+](https://marketplace.visualstudio.com/items?itemName=alygin.vscode-tlaplus): Lighter weight IDE with textfile based model configuration. Generally more responsive. The latest alpha version, [found here](https://github.com/tlaplus/vscode-tlaplus/releases), has a debugger that might be useful to you while getting a handle on the language. ## Tools for TLA+ display - [tla2json](https://github.com/japgolly/tla2json): Used to turn trace output from toolbox into json for automatic trace widget generation. - [LaTex](https://www.latex-project.org/): Used to render the LaTex output of TLA+ to dvi format. - [dvisvgm](https://dvisvgm.de/): Used to convert the dvi formatted TLA+ specifications into SVGs that could be displayed inline in the website. - [Code highlighting](https://github.com/ElliotSwart/practicalformalmodeling/blob/initial/_plugins/tla.rb): Improved / repackaged for Jekyll, but the majority of the code came from [this pull request](https://github.com/rouge-ruby/rouge/pull/1740) by Tom Lee. ## Tools for the website - [Jekyll](https://jekyllrb.com/): A static site generator. This website heavily relied on the templating functionality Jekyll provides. - [PlantUML](https://plantuml.com/): Used to generate UML diagrams from text. - [Kramdown::PlantUml](https://github.com/SwedbankPay/kramdown-plantuml): Allows rendering it in Jekyll. - [Just the Docs](https://just-the-docs.github.io/just-the-docs/): The theme that was used and modified for this website. ## Assets - [Favicon](https://www.flaticon.com/free-icons/diamond): Diamond icons created by Vaadin - Flaticon. ",
    "url": "http://localhost:4000/pragmaticformalmodeling/tools/",
    "relUrl": "/tools/"
  }
}
